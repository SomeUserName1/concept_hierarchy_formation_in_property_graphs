\documentclass[rgb]{beamer}
\usetheme{Konstanz}
\setcounter{secnumdepth}{3}
\format{169}


\begin{document}

\setmainfont{Arial}
\setsansfont{Arial}
\usebeamerfont{normalfont}


\title{Bachelor-Project: A Survey on Hierarcical Clustering Methods}
\titleCorporateDesign{Bachelor-Project}{A Survey on}{Hierarcical Clustering Methods}{}
\author{Fabian Klopfer} 
\date{\today}
\institute{Databases and Information Systems Chair \\ University of Konstanz}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}[t]{Table of Contents}
	\tableofcontents
\end{frame}

\section{Introduction}

    \subsection{Motivation}
        \begin{frame}[t]
        \subsectionpage
        Issues:
            \begin{itemize}
                \item Neo4J cardinality estimation uses independence and uniformity assumption \\
                    $\Rightarrow$ Yields non-optimal query plans
                \item Missing labels \& other data impurities \\
                    $\Rightarrow$ Yield incorrect or incomplete results
            \end{itemize}{}
            \vspace{1cm}
        Try to resolve by inferring a hierarchy of labels or label set types \\
        \end{frame}
        



    \subsection{Problem Definition}
        \begin{frame}[t]
            \subsectionpage
            Given: Set of labels $\{ l_i, l_j, l_k, \dots \}$ \\
            Wanted: Taxonomy/Hierarchy of labels  \\
            \vspace{1cm}
                    Desired capabilities:
            \begin{itemize}
                \item infer missing labels
                \item deal with outliers 
                \item cut the hierarchy tree in robust only sub-trees
                \item distance measure between instances
                \item infer correct taxonomy with samples only
                \item leverage graph edges/relationships
            \end{itemize}
        \end{frame}
        
        \begin{frame}[t]{Running Example}
        Simple example, with no overlapping labels, mapped to synthetic data (l1, l2, l11, l12, $\dots$):\\
        
        \vspace{1cm}
        
        \centering\begin{tabular}{|c|c|} \hline
             Node.name & Node.labels \\ \hline
            Fernando's & restaurant, italian \\ \hline
            Arche & restaurant, vietnamese \\ \hline
            Bangkok & restaurant, thai \\ \hline
            CampusCafe & cafe, wifi \\ \hline
            Endlicht & cafe, latenight \\ \hline
            Pano & cafe, breakfast \\ \hline
        \end{tabular}
        \end{frame}{}

\section{Solutions}
    \subsection{Approaches}
        \begin{frame}
        \sectionpage
        \subsectionpage
        \vspace{1cm}
        3 different approaches were considered:
        
        \begin{enumerate}
            \item Plain hierarchical (agglomerative) clustering
            \item Two-step clustering: First some*, then agglomerative clustering algorithms (e.g. HDBScan)
            \item Conceptual clustering
        \end{enumerate}
        \footnotesize{* Explained in some slides}
        
        Metric for how close the results are: Tree Edit Distance
        \end{frame}
        
    \begin{frame}{Plain hierarchical clustering}
        Merge the two clusters with the smallest smallest/highest/average node distance per cluster
            \begin{itemize}
                \item P1: 1 merge per level $\Rightarrow$ not a proper tree with levels; What is the appropriate amountof levels?
                \item P2: Merges are always between two clusters $\Rightarrow$ no more than 2 children in a dendrogram
                \item P3:breaks down immediately when introducing noise (e.g. remove the cafe label from campus cafe)
                \item Might be solved by flattening and n-way merging
            \end{itemize}
        \end{frame}
        
    \begin{frame}{Two-step clustering}
        Idea: reduce number of clusters and then do hierarchical clustering
            \begin{itemize}
                \item Again P1 \& P2 \& P3: 
                \begin{itemize}
                    \item P1 gets more tractable due to a removed no of merges
                    \item P2 still there but flattening the smaller tree is again more tractable
                    \item P3 highly dependent on the first clustering algorithm
                \end{itemize}
                \item P4: First step clustering algorithms are highly dependent on hyper-parameter tuning: \\
                Easy for one set, possible for some using random search, but takes a long time and gives no guarantees
                \item Might be solved by Randomized search (score for clustering), n-way merging and flattening (now easier)
                \item Other approach: recursively apply clustering procedure (again hyper-parameter tuning)
            \end{itemize}
        \end{frame}

\section{Evaluation}
    \subsection{Setup}
        \begin{frame}
        \subsectionpage
            \begin{itemize}
                \item experiments
            \end{itemize}
        \end{frame}{}

    \subsection{Results}
        \begin{frame}
        \subsectionpage
            \begin{itemize}
                \item table
                \item graphs
            
            \end{itemize}
        \end{frame}{}

\section{Conclusion}
\begin{frame}{What to acheive with thesis}
    \begin{itemize}
        \item Best of Both Worlds: Multiple, robust hierarchies \& Concrete Tree Structure 
        \item use graph structure to make more robust
    \end{itemize}
\end{frame}{}



\end{document}