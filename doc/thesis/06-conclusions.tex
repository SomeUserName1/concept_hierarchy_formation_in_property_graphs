\chapter{Conclusion}\label{\positionnumber}
\section{Summary}\label{\positionnumber}
In this thesis, a pipeline was designed in order to conduct a survey, comparing hierarchical agglomerative, density-based, partition-based and conceptual state-of-the-art clustering. More practically relevant problems with those approaches include parameter inference, space complexity and empty cluster descriptions (that is the set of all common attributes of the instances in the cluster) for noisy domains. Those could be overcome by introducing a more sophisticated parameter inference mechanism as randomized search e.g. stochastic variational inference~\cite{hoffman2013stochastic}, by implementing the computation of the distance matrix and the algorithms using it in a fashion similar to the two-way merge sort algorithm, i.e.~only using a few buffer pages keeping in ram only what is necessary for a batch of iterations and by introducing a probabilistic concept description using a fuzzy intersections for determining the cluster description. A model-based hierarchical framework namely conceptual clustering provided superior abilities regarding some characteristics, especially that the algorithm is to some degree able to deal with mixed data, has linear space complexity, is incremental and produces hierarchies by design instead of the multi-phase approach requiring additional post-processing as for the other algorithm classes. \\
This approach was used in the second part, where a graph-aware version of the previous pipeline was introduced that leverages graph-based features of the graph. Different feature vectors were evaluated, concluding that finding the features extracted have to be chosen adaptively. For example it is desirable to look at the labels and structural features only in case if there are no properties and all relationships have the same type. In the case when there are no relationships one might want to consider the properties of the node and the label as feature vector (i.e.~no structural features as they contain no information) and so on.

\section{Future Work}\label{\positionnumber}
Generally there are three categories of future work 
\begin{enumerate}
    \item Incorporate extensions to the conceptual clustering framework
    \item Find better feature vectors and ways to construct them adaptively
    \item Apply conceptual clustering or multi-phase clustering with adaptive feature vectors to other domains
\end{enumerate}{}
There are a lot of extensions to the conceptual clustering framework. Cheeseman~\cite{cheeseman1988autoclass} integrates the Bayesian interpretation more concretely into the method. Gennari~\cite{classit} reduces the number of nodes in the tree by storing pointers to the data instances instead of nodes and probabilistic concepts in the tree such that the leaves are rather the elements of a concept class than concepts themselves. Thompson et al.~\cite{thompson1991concept} takes structured domains into account and introduces internal links in the hierarchy. That is if an object from a different class is contained in the description, it may be linked and treated as entity in the hierarchy (that is a sub-concept is also part of the super-concept). The mathematical field of formal concept analysis provides a concise mathematical foundation for the framework, which Godin et al.~\cite{doi:10.1111/j.1467-8640.1995.tb00031.x} synthesised into the construction of the concepts using Gallois lattices. Xie et al.~\cite{xie2002concept} suggests the usage of the of concept lattices for classification which is similar to what Godin et al.~\cite{doi:10.1111/j.1467-8640.1995.tb00031.x} proposes. Devaney et al.~\cite{devaney1997efficient} provide an extension that focuses on the efficient selection of features. Fisher~\cite{fisher1996iterative} provides a series of improvements, that may be applied to select only features that assist the optimization process to avoid noisy concept descriptions and methods to refine the tree after an instance or all instances have been incorporated like restructuring the hierarchy or cleaning some concept descriptions. Many other improvements can be thought of like the detection of correlated features after incorporating new instances. Even tough conceptual clustering has not received much attention since the 90s it remains a field of research. \\

Finding better feature vectors includes not only the invention of better features but also the selection of the significant features and pruning of unnecessary or grouping of highly correlated features. Depending on the application use case, the adaption strategy may be far from trivial. When the features are not human readable but generated as an intermediate or internal representation as the filters that are learned during the training of a neural net, the selection of features not only from the top level but through out the network with keeping at the same time a modest number of features is an example for a non-trivial adaption procedure. \\
In the database environment, many products already contain a profile that is used for cardinality optimization which can give concrete guidelines for the features to use or not to use. \\

Cardinality estimation for query optimization has been an ongoing research topic since the invention of databases. Many different methods have been invented and applied, most of which can be divided in 2 general approaches:
\begin{itemize}
    \item Database Profile: Keep statistics about the value distributions to estimate cardinality (e.g. using general statistics about the relations, histograms, results of previous queries, $\dots$)
    \item  Sampling techniques: Execute the query with a sample of the considered relationships to
estimate cardinalities
\end{itemize}
The first approach assumes uniformity and independence assumptions when using general statistics and tries to mitigate inaccuracies by those assumptions with histograms, which accuracy depends on the chosen bucket size and which are not able to capture correlations between different attributes. Those two approaches have also been adapted to graph-based data models. 
Neumann et al. propose characteristic sets, that is the set containing all relationship types, predicates, labels or however they are called in the implementation to estimate cardinalities~\cite{neumann2011characteristic}. \\
Neo4j, the ``leading`` graph database according to itâ€™s homepage, uses a database profile with the following statistics according to the operations manual~\cite{neo4jstatistics}:
\begin{enumerate}
    \item The number of nodes having a certain label.
    \item The number of relationships by type.
    \item  The number of relationships by type, ending or starting from a node with a specific label.
    \item Selectivity per index.
\end{enumerate}
Thus they make use of the uniformity and independence assumption and rely on indices to
provide information about property value information per label~\cite{neo4jIndex}. Further the Query Planer of Neo4j can only use one index at a time and picks a sub-optimal index sometimes~\cite{neo4jBadindex}.
Those and other weaknesses in cardinality estimation may be overcome by explicitly storing taxonomies of the data implicated in the database. Taxonomies capture differences in value distributions for similar data instances and assign those to categories which may then be used not only for cardinality estimation but also for data exploration, profiling and analysis. A feature vector that is adapted according to the already existing database profile may greatly improve the results, capturing concrete attribute-value distributions for different compositions and correlations. As Cobweb is incremental this may be implemented as a dynamic (update-able) index which is then used by the query optimizer during cardinality estimation. \\
Another interesting application is that of using taxonomies of sub-graphs as self-organizing associative index, that segments the physical storage of the data base to optimize query times in a molecule database~\cite{levinson1984self}. \\

\noindent Other use cases include cognitive architectures~\cite{langley1990integrated} and the modelling of human learning behaviour~\cite{maclellan2016trestle}. 


