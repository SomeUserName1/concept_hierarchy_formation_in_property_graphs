\chapter{Appendix}

\section{An Introduction to Formal Concept Analysis}\label{\positionnumber}
Lattice Theory and Formal Concept Analysis, the latter developed by Wille~\cite{wille1982restructuring}, has been applied to solve and analyse a wide range of problems e.g. data retrieval, text mining , functional dependency discovery and association rule mining ~\cite{poelmans2012text, soergel1967mathematical, spoerri1993infocrystal, godin1993experimental, godin1993building, carpineto2004exploiting, carpineto2003mining, ganascia1987charade, oosthuizen1988induction, xie2002concept}. \\
In order to formalize the description of Objects holding key-value pairs one can use fomral concept analysis. Based on lattice theory, formal concept analysis defines a mathematical framework to exactly describe how objects and attributes may be ordered and visualized by Hasse diagrams. The following terminology is a summary of \cite{ganter2012formal}. \\

Let $O, A$ be sets, $I \subseteq (O \times A)$ a relation. We call \textbf{$O$ the set of objects, $A$ the set of attributes and $I$ the composition relation}, assigning attributes to an object\note{As a pair $(o, a)$ either is or is not in $I$ the relation is binary, i.e. Object has Attribute is the only thing we can express by this. In short: The values of attributes here are one-valued or in other words binary (present or not)}. \\

Then the \textbf{formal context $\mathcal{K}$} is given by the triple $\mathcal{C} = (O, A, I)$. 

Let $O_0 \subseteq O$ be a subset of all objects and $A_0 \subseteq A$ be a subset of all attributes then:
\[ A' = \{ a \in A | \forall o \in O_0: oIa \}  \]
the set of attributes, that the objects in $O_0$ have in common and 
\[ O' = \{ o \in O | \forall a \in A_0: oIa \} \]
the set objects having all attributes in $A_0$. \\

A \textbf{formal Concept $\mathcal{C}$} of the context $\mathcal{K}$ is a pair $(O_i, A_i)$ with $O_i \subseteq O$,  $A_i \subseteq A$ and $O_i = O'$ with respect to $A_i$ and $A_i = A'$ with respect to $O_i$. $O_i$ is called the extent and $A_i$ is called the Intent of the concept $\mathcal{C}$. \\

Let $\mathcal{C}_0 = (O_0, A_0), \mathcal{C}_1 =(O_1, A_1)$ formal concepts of context $\mathcal{K}$.\\
$\mathcal{C}_0$ is called \textbf{a sub-concept} of $\mathcal{C}_1$, iff $O_0 \subseteq O_1 \wedge A_0 \subseteq A_1$ and we call $\mathcal{C}_1$ the \textbf{super-concept} of $\mathcal{C}_0$. The relation $\leq$ is called hierarchical order and we write $\mathcal{C}_0 \leq \mathcal{C}_1$.  \\
The \textbf{concept lattice $\mathfrak{B}(\mathcal{K})$} is the set of all concepts of the context $\mathcal{K} =(O, A, I)$ ordered by the relation $\leq$. \\

\fig{img/table_hasse.png}{tablehasse}{Tabular representation of a formal context \cite{ganter2012formal}}{0.6} 

A \textbf{multi-valued Context $\mathcal{K_V} = (O, A, V, I)$} is a context with $O, A$ as in the definition of a simple Context $\mathcal{K}$ above, $V$ is the set of all values and $I \subseteq O \times A \times V$. The values $V$ of the multi-valued context $\mathcal{K_V}$ can be partitioned per attribute and interpreted in a meaningful way using a method called conceptual scaling\note{Conceptual scaling creates a context for each attribute and defines a table that map the possible attributes onto themselves in order to add additional ordering semantics to e.g. cover ordinal, nominal and convex-ordinal scales. The latter corresponds to orderings on the expected values of random variables in stochastic processes.}, which is beyond the scope of this thesis. Just notice that one can easily derive a tabular description by replacing each attribute with the pairs $\forall v_a \in V_a: (a, v_a)$ with $V_a$ of possible values of the attribute $a$. \\

\fig{img/hasse.png}{hasse}{A Hasse-diagram for the above table \cite{ganter2012formal}}{1}
A context is easily visualizable by a table as in fig. \ref{fig:tablehasse}. Fig. \ref{fig:hasse} shows the Hasse diagram for the example context from the table above. It is the visualization of the concept lattice of the context and maybe intuitively interpreted as a two sided hierarchy (directly following from using Galois connections in the lattice): The top node contains all elements and only attributes that are common to all objects. Descending in the diagram, one gets less objects and larger, more specific attribute sets. If one reads the diagram bottom up, the bottom-most node contains all attributes and either no object or the ones having all attributes Ascending the trees constructs increasingly large clusters of less and less specific objects. Breaking The diagram into two separate that end upon having either just one attribute or one object would yield two hierarchies: 
\begin{itemize}
    \item \textbf{Common attributes per object set}\note{Starting from the top-most node the common attribute set is $a$ as all objects have $a$ in common.The rightmost node shows the common attributes for objects $5, 6$.}: Focusing on the Objects and their increasingly larger common attribute sets with decreasing object size. One could formulate this as expected attributes objects have given the concept they're hosted in.
    \item \textbf{Selectivity per attribute Set}:\note{E.g. The selectivity of all attributes in conjunction is $\frac{0}{8}$, the selectivity of $a$ and $d$ is $\sigma_{a \wedge d} = \frac{4}{8} = 0.5$ and the selectivity of a is $\sigma_a = \frac{8}{8} = 1$.}. Focusing on the attributes, starting with objects that have all attributes and ending with the selectivities of single attributes or the expected number of objects, when querying for a set of attributes
\end{itemize}


The definitions in Formal Concept Analysis are too strict in some places, that's why we're going to relax them a bit to fit to the clustering problem.~\cite{ignatov2012concept}

\section{Other Approaches to Clustering}
\begin{itemize}
    \item \textbf{Grid-based methods} construct a grid on the space to cluster that may be scaled to produce clusters in multiple resolutions. Most grid-based methods have a fast processing time as they are typically independent of the number of data objects but rather on the space the objects span.
    
    \item \textbf{Bi-clustering methods} sometimes also referred to as subspace clustering clusters at the same time the objects and one class of attributes (or in the terms of Formal Concept Analysis all attributes of in the same scaling context). It is closely related to formal concept analysis as described by\cite{ignatov2012concept} and may be thought of as grouping both the columns and the rows of a table concurrently. It is a very common technique in clustering gene expression data~\cite{PONTES2015163}. An example of is given in fig. \ref{fig:biclust}. \fig{img/bicluster.jpg}{biclust}{An example of biclustering on gene expression data~\cite{PONTES2015163}.}{0.5}

    \item \textbf{Evolutionary or genetic methods} are search-based, searching the space by 
        \begin{enumerate}
            \item applying random alternations (mutation) or combinations of previous clusters to generate new clustering candidates.
            \item evaluating the so generated clustering candidates against the objective function.
            \item selecting a subset of the evaluated candidates, possibly introducing randomness and other tools to sample from different regions of the search space in order not to converge into a local minimum.
            \item iterate until there is convergence or stopping criteria are satisfied.
        \end{enumerate}
        All of these steps may be adapted to a certain setting. It is a common approach to combine evolutionary algorithms with other techniques e.g. self-organizing maps with evolutionary algorithms~\cite{leng2006design}\note{generally neural networks and evolutionary algorithms or similar constructed markov decision processes (reinforcement learning) are often combined into neural architecture search systems, using stochastic processes to select and vary the already evaluated networks to generate new ones with faster convergence rates to better AUC-scores\cite{kandasamy2018neural}}. Evolutionary algorithms may be seen as Markov chains, as the Markov property holds (each generation only depends on the last one) and each generative operator creates a new candidate solution with each candidate solution having a certain probability depending on the nature of the operation and the probabilities of all generatable candidate solutions of 1 per operator and generation~\cite{Nix1992}.
\end{itemize}

\section{Pseudo-Code for Chapter 3: Algorithms}
\subsection{Hierarchical Clustering}
\subsubsection{Hierarchical Agglomerative Clustering}
\begin{algorithm}[htp]
    \KwIn{A distance matrix $dM$ of shape $|O| \times |O|$}
    \Parameters{Distance function d}
    \KwOut{A linkage matrix $lM$}
    \hrulealg
    \Begin{
        List cluster $\rightarrow$ initializeObjectsAsOwnCluster(M)\;
        \While{cluster.size() > 2}{
            m1, m2 $\rightarrow \text{argmin}_{r_1, r_2 \in \text{dM}}d(r_1, r_2)$\;
            cluster $\rightarrow (cluster \setminus m1 \setminus m2) \cup (m_1 \cup m_2)$
            dM $\rightarrow$ computePairwiseDistance(cluster)\;
        }
        \Return cluster\;
    }
\caption{Hierarchical Agglomerative Clustering}\label{agglo}
\end{algorithm}
\subsubsection{Robust Single Linkage}
\begin{algorithm}[htp]
    \KwIn{A distance matrix $distM$ of shape $|O| \times |O|$}
    \Parameters{$k$ the number of neighbouring points, $\alpha$ the inverse density}
    \KwOut{A linkage matrix $lM$}
    \hrulealg
    \Begin{
        Stack Clustering = new Stack\;
        \For{$o_i \in O$}{
            $r_k[o_i] = inf(\{r | k \leq |B(o_i, r)| \})$\;
        }
        $r \rightarrow 0$\;
        \While{Clustering.peek().size()$ > 1$}{
            Construct $G_r = (V_r, E_r)$ with $V_r = \{x_i \in X | r_k(x_i) \leq r\}$ and $E_r = \{i \neq j: (x_i, x_j) \in X \times X | \ ||x_i - x_j|| \leq \alpha r\}$\;
            Clusterings.push(connectedComponents($G_r$))\;
            $r++$\;
        }
        \Return Clustering\;
    }
\caption{Robust Single Linkage Clustering}\label{robust}
\end{algorithm}

\subsection{Partition-based Methods}
\subsubsection{K-Means}
\begin{algorithm}[htp]density based spatial clustering 
    \KwIn{A Set of data objects $O$}
    \Parameters{$k$ the number of clusters}
    \KwOut{A set of $k$ clusters}
    \hrulealg
    \Begin{
        \For{$i = 0, \dots, k-1$}{
            Centroid $c_k \leftarrow$ Chose $k$ objects at random\;
        }
        \While{Objective function did not converge}{
            \For{$o_i \in O\setminus \{c_1, \dots c_k\}$}{
                Centroid closestCentroid = $\{c_i \in \{c_1, \dots, c_k\} | \text{argmin}_{c_i \in \{c_1, \dots, c_k\}} d(o_i, c_i)\}$
                Cluster($o_i$) = Cluster(closestCentroid)\;
            }
            \For{C $\in$ Clusters}{
                Update Centroids to the means of all objects in the cluster per feature\;
            }
        }
        \Return Clusters\;
    }
\caption{k-Means Clustering}\label{kmeans}
\end{algorithm}
\subsubsection{TTSAS}
\begin{algorithm}[htp]
    \KwIn{A Set of data objects $O$}
    \Parameters{$\Theta_1$ the maximal distance threshold, $\Theta_2$ the minimal distance threshold}
    \KwOut{A set of $k$ clusters}
    \hrulealg
    \Begin{
        \For{$o_i \in O$}{
            categorized[$o_i$] = 0\;
        }
        k = 0 \tcp*[r]{The number of clusters}
        prev = 0    \tcp*[r]{previous iteration's change}
        curr = 0    \tcp*[r]{current iteration's change}
        exist = 0   \tcp*[r]{overall change}
        \While{$\exists o_i \in O:$ categorized[$o_i$] == 0}{
            \For{i = 1 to N}{
                \uIf{categorized[$o_i$] == 0 $\wedge$ exist$ == 0 \wedge o_i$ is the first element in the new iteration of the while loop }{
                k $\leftarrow$ k + 1\;
                $C_m = \{o_i\}$\;
                categorized[$o_i$] = 1\;
                curr = curr + 1\;
                }
                \uElseIf{categorized[$o_i$] == 0}{
                    min\_dist, min\_dist\_cluster = $\{d(o_i, C_i), C_i | \text{argmin}_{C_i \in \{C_0, \dots, C_{k- 1}\}} d(o_i, C_i)\}$\;
                    \uIf{min\_dist $< \Theta_1$}{
                        min\_dist\_cluster = min\_dist\_cluster $\cup \{o_i\}$\;
                        categorized[$o_i$] = 1\;
                        curr = curr + 1\;
                    }
                    \ElseIf{min\_dist $> \Theta_2$}{
                        k $\leftarrow$ k + 1\;
                        $C_m = \{o_i\}$\;
                        categorized[$o_i$] = 1\;
                        curr = curr + 1\;
                    }
                }
                \ElseIf{categorized[$o_i$] == 1}{
                    curr = curr + 1\;
                }
            }
            exists\_change = |current\_change - previous\_change|\;
            previous\_change = current\_change\;
            current\_change = 0\;
        }
        \Return clusters\;
    }
\caption{Two-Threshold Sequential Algorithmic Scheme Clustering}\label{ttsas}
\end{algorithm}

\subsection{Density-based Methods}
\subsubsection{DBSCAN}

\begin{algorithm}[htp]
    \KwIn{A Set of data objects $O$, $o_i$ the currently expanded point, cluster[] the array storing the object to cluster mapping}
    \Parameters{$\epsilon$ the radius of the neighbourhood, MinObjects the minimal number of neighbours of a core point, cluster\_id the id of the cluster to be used}
    \KwOut{A boolean indicating weather a new cluster was formed}
    \hrulealg
    \Begin{
    seeds $\leftarrow \{o_j \in O\setminus \{o_i\} | d(o_i, o_j) \leq \epsilon \}$\;
       \uIf{seeds.size < MinObjects}{
            \Return False\;
       }
       \Else{
            \For{seed in seeds}{
                cluster[seed] = cluster\_id\;
            }
            \While{seeds $\neq \emptyset$}{
                currentSeed $\leftarrow$ seeds.pop()\;
                currentSeedNeigh = $\leftarrow \{o_j \in O\setminus \{\text{currentSeed}\}| d(o_i, o_j) \leq \epsilon \}$\;
                
                \If{currentSeedNeigh.size $\geq$ MinObjects}{
                    \For{$i = 1, \dots, $ currentSeedNeigh.size}{
                        currentResult = currentSeedNeigh[i]\;
                        \If{cluster[currentResult] == -1 $\vee$ cluster[currentResult] == 0 \tcp*[r]{i.e. currentResult is unclassified or noise}}{
                            cluster[currentResult] = cluster\_id\;
                            \If{cluster[currentResult] == -1 \tcp*[r]{unclassified}} {
                                seeds.append(currentResult)\;
                            }
                        }
                    }
                }
            }
            \Return True\;
       }
    }
\caption{Expand Cluster method}\label{expand_cluster}
\end{algorithm} 

\begin{algorithm}[htp]
    \KwIn{A Set of data objects $O$}
    \Parameters{$\epsilon$ the radius of the neighbourhood, MinObjects the minimal number of neighbours of a core point}
    \KwOut{A set of $k$ clusters}
    \hrulealg
    \Begin{
        \For{$o_i \in O$ }{
            cluster[$o_i$] $\leftarrow$ -1 \tcp*[r]{All objects are unclassified at the start}
        }
       cluster\_id = nextId(NOISE)\;
       \For{$o_i \in O$}{
            \If{cluster[$o_i$] == -1}{
                \If{expand\_cluster($O$, $o_i$, cluster, cluster\_id, $\epsilon$, MinObjects)}{
                    cluster\_id = nextInt(cluster\_id)\;
                }
            }
       }
    }
\caption{Density-based Spatial Clustering of Applications with Noise}\label{dbscan}
\end{algorithm}
\subsubsection{OPTICS}

\begin{algorithm}[htp]
    \KwIn{A Set of data objects $O$, $o_i$ the currently expanded point}
    \Parameters{$\epsilon$ the radius of the neighbourhood, MinObjects the minimal number of neighbours of a core point, OrderedFile the file to write the cluster ordering to}
    \KwOut{None}
    \hrulealg
    \Begin{
    seeds $\leftarrow \{o_j \in O\setminus \{o_i\} | d(o_i, o_j) \leq \epsilon \}$\;
    $o_i$.processed $\leftarrow$ 1\;
    $o_i$.reachability\_distance = UNDEFINED\;
    $o_i$.setCoreDistance(seeds, $\epsilon$, MinObjects)\;
    OrderedFile.write($o_i$)\;
       \If{$o_i$.core\_distance $\neq$ UNDEFINED}{
       OrderSeeds.update(seeds, $o_i$)\;
            \While{OrderSeed $\neq \emptyset$}{
                currentSeed $\leftarrow$ seeds.next()\;
                seeds = $\leftarrow \{o_j \in O\setminus \{\text{currentSeed}\}| d(o_i, o_j) \leq \epsilon \}$\;
                currentSeed.preprocessed = 1\;
                currentSeed.setCoreDistance(seeds, $\epsilon$, MinObjects)\;
                OrderedFile.write(currentSeed)\;

                \If{currentSeed.core\_distance $\neq$ UNDEFINED}{
                    OrderSeeds.update(seeds, currentSeed)\;
                }
            }
       }
    }
\caption{Expand Cluster Order method}\label{expand_cluster_order}
\end{algorithm}

\begin{algorithm}[htp]
    \KwIn{neighbours a set of neighbour objects, $o_i$ the current center object}
    \Parameters{None}
    \KwOut{None}
    \hrulealg
    \Begin{
        c\_dist = $o_i$.core\_distance\;
        \For{$o_j \in$ neighbours}{
            \If{$o_j$.processed == 0}{
                new\_r\_dist = max(c\_dist, $o_i$.dist($o_j$))\;
                \uIf{ $o_j$.reachability\_distance == UNDEFINED}{
                    $o_j$.reachability\_distance = new\_r\_dist\;
                    insert($o_j$, new\_r\_dist)\;
                    }
                \Else{
                    \If{ new\_r\_dist $< o_j$.reachability\_distance}{
                        $o_j$.reachability\_distance = new\_r\_dist\;
                        decrease($o_j$, new\_r\_dist)\;
                    }
                }
            }
        }
    }
\caption{OrderSeeds.update method}\label{order_seeds_update}
\end{algorithm}

\begin{algorithm}[htp]
    \KwIn{A Set of data objects $O$}
    \Parameters{$\epsilon$ the radius of the neighbourhood, MinObjects the minimal number of neighbours of a core point, OrderedFile a file to write to}
    \KwOut{An Ordering of the clusters in the database with different densities} 
    \hrulealg
    \Begin{
        OrderedFile.open()\;
        \For{$o_i \in O$ }{
            $o_i$.processed $\leftarrow$ 0\;
        }
        \For{$o_i \in O$}{
            \If{processed[$o_i$] == 0}{
                expand\_cluster\_order($O$, $o_i$, $\epsilon$, MinObjects, OrderedFile)\;
            }
       }
       OrderedFile.close()\;
    }
\caption{Ordering Points To Identify Cluster Structure}\label{optics}
\end{algorithm}
\subsubsection{HDBSCAN}
\begin{algorithm}[htp]
    \KwIn{A Set of data objects $O$}
    \Parameters{MinObjects the minimal number of neighbours of a core point}
    \KwOut{A dendrogram}
    \hrulealg
    \Begin{
        \For{$o_i \in O$ }{
            copute the core distance wrt. MinObjects\;
        }
        Build the mutual reachability graph $G_{\text{MinObjects}}$\;
        Build the minimum spanning tree of $G_{\text{MinObjects}}$\;
        Add a self-edge for each vertex $v_{o_i}$ in the minimal spanning tree with weight $d_{\text{core}}(o_i)$\;
        
        Assign all objects the same label \tcp*[r]{this forms the root of the dendrogram}
        \While{$G_{\text{MinObjects}}$ has edges}{
            Set the dendrogram scale value of the current level in the hierarchy to the largest edge weight which is to be removed\;
            Remove the largest edge(s)\;
            Assign a label to the connected component, that contained the end vertices of the removed edge(s)\;
            If the end vertices are not in a connected component assign the noise label to those\;
        }
    }
\caption{Hierarchical Density-based Spatial Clustering of Applications with Noise}\label{hdbscan}
\end{algorithm}

\subsection{Conceptual Clustering}
\subsubsection{Cobweb}
\begin{algorithm}[htp]
    \KwIn{An object $o_i \in O$}
    \Parameters{node the node currently visiting}
    \KwOut{A hierarchy of clusters with concept descriptions}
    \hrulealg
    \Begin{
        updateCounts(node, $o_i$)\;
        \uIf{node is a leaf}{
            node.add\_child($o_i$)\;
        }
        \Else{
            host, host\_cu = find the child of node, that best hosts $o_i$\;
            new\_class\_cu = probeCreateNewClass($o_i$)\;
            merge\_cu = probeMerge(host, $o_i$)\;
            split\_cu = probeSplit(host)\;
            max\_cu = $\max(\text{new\_class\_cu}, \text{merge\_cu}, \text{split\_cu}, \text{host\_cu})$\;
            \uIf{max\_cu $==$ new\_class\_cu}{
                createNewClass($o_i$)\;
            }
            \uElseIf{max\_cu $==$ merge\_cu}{
                merged\_node = merge(host, $o_i$)\;
                COBWEB($o_i$, merged\_node)\;
            }
            \uElseIf{max\_cu == split\_cu}{
                split(host)\;
                COBWEB($o_i$, node)\;
            }
            \Else{
            COBWEB($o_i$, host)
            }
        }
    }
\caption{COBWEB}\label{cobweb}
\end{algorithm}
\section{Taxonomy Extraction example}
\begin{minipage}{0.4\textwidth}
$C_{14} = C_0 \cup C_2$, with distance 1 \\
$C_{15} = C_1 \cup C_14$, with distance 1 \\
$C_{16} = C_3 \cup C_15$, with distance 1 \\
$C_{17} = C_5 \cup C_7$, with distance 1 \\
$C_{18} = C_6 \cup C_17$, with distance 1 \\
$C_{19} = C_4 \cup C_18$, with distance 1 \\
$C_{20} = C_8 \cup C_19$, with distance 1 \\
$C_{21} = C_9 \cup C_11$, with distance 1 \\
$C_{22} = C_12 \cup C_21$, with distance 1 \\
$C_{23} = C_13 \cup C_22$, with distance 1 \\
$C_{24} = C_16 \cup C_20$, with distance 2 \\
$C_{25} = C_23 \cup C_24$, with distance 2 \\
$C_{26} = C_10 \cup C_25$, with distance 2 \\
\end{minipage} \hfill
\begin{minipage}{0.4\textwidth}
$[[ 0.,  2.,  1.]$ \\
 $[ 1., 14.,  1.]$\\
 $[ 3., 15.,  1.]$\\
 $[ 5.,  7.,  1.]$\\
 $[ 6., 17.,  1.]$\\
 $[ 4., 18.,  1.]$\\
 $[ 8., 19.,  1.]$\\
 $[ 9., 11.,  1.]$\\
 $[12., 21.,  1.]$\\
 $[13., 22.,  1.]$\\
 $[16., 20.,  2.]$\\
 $[23., 24.,  2.]$\\
 $[10., 25.,  2.]]$ \\
\end{minipage} \\

\begin{minipage}{0.4\textwidth}
$[$\\
    $[[ 0.,  2.,  1.,  3.], 1]$\\
    $[[ 5.,  7.,  6.,  4.,  8.], 1]$\\
    $[[ 9., 11.,  12.,  13. ], 1]$\\
    $[[16.,  20.,  23.,  10.], 2]$  \\        
$]$\\
\end{minipage}  \hfill
\begin{minipage}{0.4\textwidth}
$C_{16} = C_0 \cup C_2 \cup C_1 \cup C_3$, with distance 1 \\  
$C_{20} = C_5 \cup C_7 \cup C_6 \cup C_4 \cup C_8$, with distance 1 \\ 
$C_{23} = C_9 \cup C_{11} \cup C_{12} \cup C_{13}$, with distance 1 \\ 
$C_{26} = C_{16} \cup C_{20} \cup C_{23} \cup C_{19}$, with distance 2 \\         
\end{minipage} 



%%%%%%% TODO: Maybe set theory here

%\section{Definitions}\label{\positionnumber}
%In order to elaborate on the methods and results, some concepts, terms and notations need to be defined in the following.
%\section{Graph Database}\label{\positionnumber}
%A database management system is a  software system that assists users to  maintain and utilize large amounts of data ~\cite{Ramakrishnan:2002:DMS:560733}.
%A \textbf{graph database management system}, henceforth graph database, is a database management system which uses a graph structure as model to represent data~\cite{neo4j_book}, e.g. the Resource Description Framework or the Property Graph Model. Fig. \ref{fig:neo4jarch} shows the high level architecture of the graph database Neo4J. \\
%\marginfigure{img/neo4j_arch.png}{neo4jarch}{High level architecture of the Graph database Neo4J. \cite{neo4j_book}}{-6cm}


