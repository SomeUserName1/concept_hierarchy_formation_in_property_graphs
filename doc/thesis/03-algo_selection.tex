\chapter{Algorithm Selection}\label{\positionnumber}
% TODO use examples from presentation & yelp dataset
\section{Problem Statement}\label{\positionnumber}
A lot of data contains implicit regularities and structure~\cite{mitchell2006discipline}~\cite{carlsson2009topology} that are not reflected explicitly by the data model.\\
When focusing on data objects\note{or data instances, data points, elements of the sample, $\dots$} or nodes in terms of the property graph model there is often an implicit hierarchy of sub-types given a label. Such an implicit type hierarchy may provide further insights about the data as it may e.g. help in estimating cardinalities during query optimization, when keeping further statistics about the latent class distribution. \\

\margincode{code/business_ex.json}{businesex}{An example business object from the yelp data set}{text}{1cm}

For example in the yelp data set, the businesses may be further categorized in e.g. restaurants, spas, shopping malls, etc. One may further subdivide the restaurants by additional properties and features like cooking style (italian, asian, burgers, $\dots$) or  by location (italian restaurants in New York, asian restaurants in Manhatten). See lst. \ref{lst:businesex} for an example object. \\
The below example is motivated by the yelp data set. In the yelp data set instances are assigned categories by users of the yelp system. Inspecting them shows frequently category label distributions like in Tab. \ref{tab:running_ex}: \\
\begin{table}[htp]
     \centering
     \begin{tabular}{c c} \toprule
            Node.name & Node.labels \\ \midrule
            Fernando's & restaurant, italian \\ 
            Arche & restaurant, vietnamese \\ 
            ZumElefanten & restaurant, thai \\ 
            CampusCafe & cafe, wifi \\ 
            Endlicht & cafe, latenight \\ 
            Pano & cafe, breakfast \\
            Lago & shopping, mall \\ 
            Seerhein Center & shopping, cheap \\ 
            Seepark & Shopping, expensive \\ \bottomrule
        \end{tabular}
    \caption{A fictive example of business and user-defined associated labels.}
    \label{tab:running_ex}
\end{table}{}
Which may be visualized as hierarchy like in fig. \ref{fig:ex_hier}. \\
\fig{img/ex_hierarchy}{ex_hier}{The hierarchy that is implicit in the labels of the example in tab.\ref{tab:running_ex}}{1} \\
 In practice the categories are just informal strings, there is no notion of this structure in the current representation of the property graph model. In order to leverage this implicit hierarchy it needs to be made explicit. \\
 
Given a property graph model-based database, how can one extract such a hierarchy?
Finding classes or in other terms subsets of the input is what is addressed by clustering methods. \\
In order to infer a classes for nodes from a property graph model-based database, one may take various information into account: 
\begin{itemize}
    \item the user-defined labels only
    \item the properties of the node
    \item per node structural features of the underlying graph
    \item the properties of the relationships
    \item further constructed features based on previous steps
\end{itemize}
As briefly mentioned in chapter 2, different algorithms yield different clusterings as well as the same clustering algorithm with different features and parameters yields different clusterings. Thus in order to find a suitable hierarchy the following steps will be subsequently performed:
\begin{enumerate}
    \item Find a suitable clustering algorithm using a minimal feature set and different parameters to cover the space of algorithms and parameters as extensively as feasible.
    \item Find a suitable feature vector leveraging the graph structure
    \item Specify a pipeline to unify the above results
\end{enumerate}
Each step has it's own requirements and objectives that are addressed in the subsequent sections. \\


\section{Overview}\label{\positionnumber}
\fig{img/pipeline.png}{pipeline}{The pipeline used to evaluate the clustering approaches}{1}
The selection of a fitting algorithm has three main requirements:
\begin{itemize}
    \item Memory usage: data bases need a certain amount of memory to work and memory is always the main performance bottleneck for databases
    \item Adaptive to nominal and numeric data: Most databases support a couple of data types, that may be summarized as either nominal (string, characters, booleans, $\dots$) or numeric (integer, long, floats, $\dots$), i.e. the clustering algorithm needs to be able to support both kinds of data.
    \item Noise detection and tolerance: As there are frequently missing fields in data records the clustering algorithm should be tolerant to noise and detect noisy instances. E.g. in the yelp data set the maintenance of the business categories and other data is dependent on the support of the community around yelp and the owner of the business.
\end{itemize}
An overview of the Algorithm evaluation pipeline can be seen in fig. \ref{fig:pipeline}
In order to evaluate a wide range of algorithms, a pipeline is used that uses the interface for clustering algorithms specified in \fullref{clustering}. \\
The interface looks then like \ref{endtoend}
\begin{algorithm}[h]
    \KwIn{Set of Objects $O$, set of sets of Attributes $A = \{A_{o_0}, A_{o_1}, \dots \}$}
    \Parameters{Parameters P to the implementation if necessary}
    \KwOut{A Taxonomy of $O$} 
\caption{Pipeline}\label{endtoend}
\end{algorithm}

\subsection{Encoding labels as vectors}
\begin{algorithm}[h]
    \KwIn{Set of Objects $O$, set of sets of Attributes $A = \{A_{o_0}, A_{o_1}, \dots \}$}
    \Parameters{No Parameters}
    \KwOut{A matrix $M$ of shape $|O| \times |A|$ with $m_{o,a} \in \{0, 1\}$ that is consistent with $I$} 
    \Begin{
        $M \rightarrow zeros(|O|, |A|)$\;
        \For{Object o in O} {
            \For{Attribute a in A}{
                \If{$a \in A_o$}{
                    $M[o,a] = 1$\;
                }
            }
        }
    }
\caption{Vectorize Labels}\label{vect}
\end{algorithm} 
Most clustering algorithms take a feature vector as input, that is a matrix of reals on some metric space. Character strings need to be transformed to comply with this requirement. Vectorization builds a matrix representation, given a list of string attributes.
As discussed in \fullref{2.1.3}, one can easily do this by constructing the relation $I$ given the set of objects $O$ and a set of attributes per object. This yields the formal context $(O, A, I)$. Taking the per object row from this table and writing per place a $(o,a) \in I \Rightarrow 1$ otherwise $0$, yields a vector.
Consider the example given in tab. \ref{tab:running_ex}. The vectorized representation of the example in tab. \ref{tab:running_ex} is given in tab. \ref{tab:vect_running_ex}
\begin{table}[htp]
 \begin{adjustwidth}{-4cm}{0cm}  
     \centering
     \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
            Node.name & rest & ital & viet & thai & cafe & wifi & late & brea & shop & mall & chea & expe \\ \hline \hline
            Fernando's      & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\  \hline
            Arche           & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\  \hline
            ZumElefanten    & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\ \hline
            CampusCafe      & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
            Endlicht        & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ \hline
            Pano            & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\ \hline
            Lago            & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\ \hline
            Seerhein Center & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 \\ \hline
            Seepark         & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\ \hline
        \end{tabular}
    \caption{A vector representation of the example from tab. \ref{tab:running_ex} corresponding to $I$}
    \label{tab:vect_running_ex}
    \end{adjustwidth}
\end{table}{} \\


\subsection{Taxonomy Extraction}
Both approaches return A hierarchically ordered set of subsets of $O$ but with only one merge per tree depth level: This is a characteristic trait of a dendrogram and the most significant difference to other trees and tree-like structures. instead of consisting of only $\log_k n$ levels it consists of $n$ levels. Thus the dendrogram can be flattened to get the corresponding "well-formed" tree of clusters - a taxonomy. \\
The output of agglomerative clustering is a linkage matrix, a matrix of shape $3 \times |merges|$, where column 0 contains the first merged cluster, column 1 the second merged cluster and column 2 the distance of the merged clusters, representing the hierarchically ordered set of subsets.
What the flattening intuitively does is aggregating merges where clusters have been merged consecutively and with the same distance \note{e.g. $C_{00} = C_0 \cup C_1$ with distance $d$, then $C_{01} = C_{00} \cup C_2$with distance $d$, then $C_{02} = C_{01} \cup C_3$ with distance $d$ would be aggregated into $C_{00} = C_0 \cup C_1 \cup C_2, \cup C_4$ with distance $d$}. 
\begin{algorithm}[h]
    \KwIn{linkage matrix $lM$}
    \Parameters{None}
    \KwOut{A taxonomy of $O$} 
    \Begin{
        previousRow $\leftarrow [-1, -1, -1]$\;
        taxonomy $\leftarrow$ new Map\;
        cluster $\leftarrow$ new Set\;
        distance $\leftarrow$ -1\;
        consecutive = False\;
        \For{row in $lM$}{
            \If{(row[0] == previousRow[0] or row[1] == previousRow[1])  and row[2] == previousRow[2]}{
                \If{taxonomy is not empty and not consecutive}{
                    taxonomy.removeLast()\;
                    cluster.add(previousRow[0], previousRow[1])\;
                }
                 distance $\leftarrow$ row[2]\;
                 cluster.add(row[0], row[1])\;
                 consecutive $\leftarrow$ True\;
            }{
                \If{consecutive}{
                    taxonomy.add(clusters, distance)\;
                    cluster.clear()\;
                    distance = -1\;
                }
                consecutive = False\;
                taxonomy.add(\{row[0], row[1]\}, row[2])
            }
        }
        return cluster
    }
\caption{Taxonomy Extraction from a Dendrogram}\label{taxo}
\end{algorithm}

\begin{figure}[hb]
    \includegraphics[width=0.49\textwidth]{img/extract_ex_dendro.png}
        \includegraphics[width=0.49\textwidth]{img/extract_ex_taxo.png}
    \caption{The Input and output of the taxonomy extraction heuristic}
    \label{fig:my_label}
\end{figure}


\section{One-Step Approach}\label{\positionnumber}
Hierarchical clustering algorithms may be divided into \textbf{agglomerative and divisive methods}~\cite{han2011data}. Agglomeraitve methods start with each data instance beeing a cluster and merges in all subsequent steps 2 clusters at a time in a bottom-up fashion. Divisive methods start with all instances beeing in the same cluster and splits them consecutively top-down. We are going to focus on the agglomerative methods as there are computational challenges inherent to divisive clustering that agglomerative clustering does not have\note{There are $2^{n-1}-1$ possible ways to partition a set of n elements into 2 sets and in divisive clustering one needs to heuristically choose a partitioning. Backtracking could improve the performance but this is not scalable and may end up in exponential runtime.\cite{han2011data}}~\cite{han2011data}. \\

\subsection{Hierarchical Agglomerative Clustering}\label{\positionnumber}
Agglomerative methods need a measure for the distance between two clusters. The most common choices are with $C_i, C_j$ clusters:
\begin{itemize}
    \item \textbf{Minimum distance}, also called \textbf{Single Linkage}: 
    \[d_{\text{min}}(C_i, C_j) = \min_{e_1 \in C_i, e_2 \in C_j} |e_1 - e_2|\]
    \item \textbf{Maximum distance}, also called \textbf{Complete Linkage}: 
    \[d_{\text{max}}(C_i, C_j) = \max_{e_1 \in C_i, e_2 \in C_j} |e_1 - e_2|\]
    \item \textbf{Average distance}, also called \textbf{Average Linkage}: 
    \[d_{\text{avg}}(C_i, C_j) = \frac{1}{|C_i| \cdot |C_j|} \sum_{e_1 \in C_i, e_2 \in C_j} |e_1 - e_2|\]
\end{itemize}
There are also the centroid distance, computing the distance from the mean elements of each cluster and Ward's method but those use the notion of the mean element (or the center of gravity of the cluster) which is not directly applicable for binary attributes\note{One could use fuzzy logic in order to provide a mean with non-crisp attributes in order to apply the other two distances~\cite{kruse2016computational}.} \cite{mirkin2013mathematical, han2011data}. A visualization of the distances is shown in fig. \ref{fig:agglo_dist}.
\fig{img/agglo_distance.png}{agglo_dist}{The different distance functions visualized}{1} \\
The algorithm proceeds as follows: 
\begin{enumerate}
    \item initialize all data objects as own clusters
    \item compute the cluster distance matrix by computing the chosen distance function pairwise for all clusters
    \item remove and merge the two clusters with the minimal distance with respect to the chosen distance function. add the new cluster to the list of clusters
    \item if there are more than 2 elements in the set of clusters continue with step 2\note{Alternatively one can specify the amount of clusters desired, but as we are interested in hierarchies here this is not desired.}. This is procedure is neither complete not always correct. For example two clusters differing in a different attribute but still having the distance 1 may be merged. There are also other operations to integrate 
\end{enumerate}
\begin{algorithm}[h]
    \KwIn{A matrix $M$ of shape $|O| \times |A|$ with $m_{o,a} \in \{0, 1\}$ that is consistent with $I$}
    \Parameters{Distance function d}
    \KwOut{A linkage matrix $lM$}
    \Begin{
        List cluster $\rightarrow$ initializeObjectsAsOwnCluster(M)\;
        \While{cluster.size() > 2)}{
            dM $\rightarrow$ computePairwiseDistance(cluster)\;
            m1, m2 $\rightarrow \text{argmin}_{r_1, r_2 \in \text{dM}}(r_1, r_2)$\;
            cluster $\rightarrow (cluster \setminus m1 \setminus m2) \cup (m_1 \cup m_2)$
        }
        return cluster
    }
\caption{Hierarchical Agglomerative Clustering}\label{agglo}
\end{algorithm}

\subsection{Robust Single Linkage}\label{\positionnumber}
% TODO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Two-Step Approach}\label{\positionnumber}
% TODO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The two step approaches first use a non-hierarchical algorithm in order to derive basic clusters in order to apply hierarchical clustering atop of the output of the non-hierarchical algorithm. This results not only in a performance gain but also yields a smaller and thus more interpretable dendrogram. In the following a subset of the evaluated algorithms is described. 
\subsection{Non-Hierarchical Clustering}
\subsubsection{K-Medians}\label{\positionnumber}
\subsubsection{TTSAS}\label{\positionnumber}
\subsubsection{Self-organizing Map}\label{\positionnumber}

\subsection{Subsequent Hierarchical Clustering}
This also yields problems: When the first clustering procedure was applied, one needs to (re-)construct a feature description of the cluster so that the hierarchical clustering algorithm can calculate the distance of clusters. One option is to take the intersection of the category sets of the elements in the clusters. Another option is to construct a probabilistic description  of the feature sets. 

\subsection{OPTICS}\label{\positionnumber}
\subsection{HDBSCAN}\label{\positionnumber}



\section{Conceptual Clustering}
\subsection{Trestle}

\section{Implementation}


\section{Evaluation}
 \subsection{Datasets}
   
  \section{Results}
    \subsection{Yelp}
    \subsection{synthetic}


